{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1155fbaf-d0e7-498a-8116-1499d74cb3ed",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In the lab, we saw how we can take a training script and use `mlflow` to submit runs. In practice, the training script isn't always clean code that is ready to use. So in this assignment, we learn to **refactor code** to train a model and prepare it for scoring. We learn about functionality in `sklearn` for **model persistence**, a fancy term for saving a model, so we can later load it and use it for prediction. We also learn how we can chain pre-processing steps and attach them to the model prediction step so we can both pre-process and score in one smooth flow.\n",
    "\n",
    "The bulk of the code that needs to execute is already given. This should look like code that we've written throughout the course. But when moving to production, it is **highly, highly recommended** that we **refactor** the code. What this means is that we need to go over the code from top to finish and do a bunch of things (now that we have the advantage of **hindsight**):\n",
    "\n",
    "- add **comments** in the code, for future us or (heaven forbid!) if someone else has to look at our code!\n",
    "- remove **extra code** that we wrote during development for debugging purposes but no longer need, or at least comment it out\n",
    "- simplify things, remove redundancies and make the code more **modular** by using functions or classes if needed\n",
    "- **parametrize** the code, so you avoid **hard-coding** things that need to change, and move them as high-level parameters at the top of the code, making it easy to change things without breaking things\n",
    "- create a runtime for the environment using Conda or PIP, which acts as a snapshot of the Python libraries we used and pins down their versions (careful: not all packages used during training are needed during scoring, and since the scoring enivironment is supposed to be **lightweight**, we should identify and remove such packages)\n",
    "- add **scaffolding**, this is to make sure that the code executes **gracefully** when errors happen, such as when the model expects a certain feature in the future data but it is missing for some reason (by gracefully, we mean that we use things like `try` and `except` to catch and redirect errors)\n",
    "- last but not least, never stop **testing**, but testing here can mean **unit testing**, **integration testing**, and even statistical tests for **data drift** or **model drift**\n",
    "- if you haven't yet (what have you been waiting for!) begin to **version control** your code using **git** or something similar\n",
    "\n",
    "Of course even with hindsight, doing these things is not easy and the answers are not always available, but we do the best we can and with experience we get better at it. Every project can be viewed as a work in progress, but applying certain **best practices** can make it easier to keep improving things without breaking them. This is what **agile development** is all about. You may have noticed that all the above steps are things we do generally when we write applications. It's just that not all data scientists have a rigorous computer science background and so we tend to have looser standards in general. The above is just the beginnig, not the end. Usually depending on the type of deployment we are doing, there are specific additional steps needed. \n",
    "\n",
    "Enough said. It's time for work! We used our knowledge of data science to write up some code to read data, pre-process it, train a model and use it to get predictions on a test data set. Here's a standard training code snippet. Examine it and run it to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65583d2-2d62-4a61-89b9-650fcdae0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are age, balance, day, duration, campaign, pdays, previous.\n",
      "Categorical columns are job, marital, education, default, housing, loan, contact, month, poutcome.\n",
      "Training data has 40689 rows.\n",
      "Test data has 4522 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurized training data has 40689 rows and 51 columns.\n",
      "Featurized test data has 4522 rows and 51 columns.\n",
      "Precision = 65% and recall = 35% on the training data.\n",
      "Precision = 63% and recall = 34% on the validation data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bank = pd.read_csv('data/bank-full.csv', delimiter = ';')\n",
    "\n",
    "num_cols = bank.select_dtypes(['integer', 'float']).columns\n",
    "cat_cols = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "print(\"Training data has {} rows.\".format(X_train.shape[0]))\n",
    "print(\"Test data has {} rows.\".format(X_test.shape[0]))\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "onehoter = OneHotEncoder(sparse = False)\n",
    "onehoter.fit(X_train[cat_cols])\n",
    "onehot_cols = onehoter.get_feature_names(cat_cols)\n",
    "X_train_onehot = pd.DataFrame(onehoter.transform(X_train[cat_cols]), columns = onehot_cols)\n",
    "X_test_onehot = pd.DataFrame(onehoter.transform(X_test[cat_cols]), columns = onehot_cols)\n",
    "\n",
    "znormalizer = StandardScaler()\n",
    "znormalizer.fit(X_train[num_cols])\n",
    "X_train_norm = pd.DataFrame(znormalizer.transform(X_train[num_cols]), columns = num_cols)\n",
    "X_test_norm = pd.DataFrame(znormalizer.transform(X_test[num_cols]), columns = num_cols)\n",
    "\n",
    "X_train_featurized = X_train_onehot # add one-hot-encoded columns\n",
    "X_test_featurized = X_test_onehot   # add one-hot-encoded columns\n",
    "X_train_featurized[num_cols] = X_train_norm # add numeric columns\n",
    "X_test_featurized[num_cols] = X_test_norm   # add numeric columns\n",
    "\n",
    "del X_train_norm, X_test_norm, X_train_onehot, X_test_onehot\n",
    "\n",
    "print(\"Featurized training data has {} rows and {} columns.\".format(*X_train_featurized.shape))\n",
    "print(\"Featurized test data has {} rows and {} columns.\".format(*X_test_featurized.shape))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(max_iter = 5000, solver = 'lbfgs')\n",
    "logit.fit(X_train_featurized, y_train)\n",
    "\n",
    "y_hat_train = logit.predict(X_train_featurized)\n",
    "y_hat_test = logit.predict(X_test_featurized)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_train = precision_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "precision_test = precision_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "recall_train = recall_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the training data.\".format(precision_train, recall_train))\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6d46d-a425-4e11-ab2e-f7c6c97980a2",
   "metadata": {},
   "source": [
    "The code above trains a model on data that contains both categorical and numeric features. We normalize the numeric features and one-hot-encode the categorical features as part of pre-processing. In the above code we do this \"manually\", however as shown [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html) we can compose data transformations and ML steps to create a **single multi-step pipeline**. The pipeline object (conviniently called `pipeline` in the docs) has a `fit` and `predict` method:\n",
    "- By calling `fit`, the raw input data is first transformed into the featurized data, and then passed to the ML algorithm to train a model.\n",
    "- By calling `predict`, the raw input data is first transformed into the featurized data (just like `fit`), and and then used to get predictions (using the model trained when we called `fit`).\n",
    "\n",
    "You can even create your own transformers and inculde them in a pipeline step, using as shown [here](https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers), but we won't worry about this for this assignment.\n",
    "\n",
    "Let's also have some data that we can use for scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11401cb-714b-41f5-9321-68b36c6e76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/new_data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/new_data.json\n",
    "{\"age\": {\"0\": 40, \"1\": 47},\n",
    " \"balance\": {\"0\": 580, \"1\": 3644},\n",
    " \"campaign\": {\"0\": 1, \"1\": 2},\n",
    " \"contact\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"day\": {\"0\": 16, \"1\": 9},\n",
    " \"default\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"duration\": {\"0\": 192, \"1\": 83},\n",
    " \"education\": {\"0\": \"secondary\", \"1\": \"secondary\"},\n",
    " \"housing\": {\"0\": \"yes\", \"1\": \"no\"},\n",
    " \"job\": {\"0\": \"blue-collar\", \"1\": \"services\"},\n",
    " \"loan\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"marital\": {\"0\": \"married\", \"1\": \"single\"},\n",
    " \"month\": {\"0\": \"may\", \"1\": \"jun\"},\n",
    " \"pdays\": {\"0\": -1, \"1\": -1},\n",
    " \"poutcome\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"previous\": {\"0\": 0, \"1\": 0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4273dc-72c7-4395-8525-18ab7dfaa3bc",
   "metadata": {},
   "source": [
    "Run through the following steps to first refactor and test code:\n",
    "\n",
    "- **Step 1:** Compose the data processing and training steps in the above code into one pipeline as shown in the referenced doc. <span style=\"color:red\" float:right>[15 point]</span>\n",
    "- **Step 2:** Call `fit` and `predict` on the pipeline to make sure that it all works. Remember to pass them the **un-processed** (original) data, since the data processing should be built into the pipeline now. <span style=\"color:red\" float:right>[5 point]</span>\n",
    "- **Step 3:** Save your pipeline object using `joblib` as shown [here](https://sklearn.org/modules/model_persistence.html). <span style=\"color:red\" float:right>[5 point]</span>\n",
    "- **Step 4:** Now write a **new script** for scoring: it loads the pipeline you saved in the last step, reads the data `../data/new_data.json` and converts it to a `pandas.DataFrame` object, and obtains predictions on it. The predictions should be stored as a `json` file `../data/new_preds.json`. <span style=\"color:red\" float:right>[10 point]</span>\n",
    "\n",
    "To begin work on the assignemnt, it's best to first copy the training script in a cell below and begin modifying it according to the instructions in steps 1-3. Then create a new cell and populating with the scoring script described in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b70e290-85e7-4867-bbe5-4aee41286af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Using cached category_encoders-2.6.1-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.7.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (0.13.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.3.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (0.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2021.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.0.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4774f06-f087-4a6f-a468-2fd458a8f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are age, balance, day, duration, campaign, pdays, previous.\n",
      "Categorical columns are job, marital, education, default, housing, loan, contact, month, poutcome.\n"
     ]
    }
   ],
   "source": [
    "## modified training script goes here\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bank = pd.read_csv('data/bank-full.csv', delimiter = ';')\n",
    "\n",
    "num_cols = bank.select_dtypes(['integer', 'float']).columns\n",
    "cat_cols = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7726817-834c-4982-871c-bdae90fbf1e1",
   "metadata": {},
   "source": [
    "##### We wanted to create pipeline for organizing our model.\n",
    "##### we have 3 parts.\n",
    "##### 1.We normalized the numrical features. and we did it by using standardscaler\n",
    "##### 2.we hotencoded the categorical features and we did it by using OneHotEncoder\\\n",
    "##### 3.we put these 2 parts in a object called preprocessor \n",
    "##### 4.Then we defined a pipeline and we joined the mentioned parts above with our logisticregression classifier.\n",
    "##### 5.Then we trained our model and did prediction based on the data which is original.\n",
    "##### 6.We printed the accuracy which is similar to the model we trained without pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dd2daf-4a3b-48ef-8889-1a08132fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = num_cols\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features =cat_cols\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57201653-3a2d-43c2-8e78-ac3a7070dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0316c17c-4a41-4e9e-8199-efa102e378a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_clf = clf.predict(X_train)\n",
    "y_hat_test_clf = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_train = precision_score(y_train, y_hat_train_clf, pos_label = 'yes') * 100\n",
    "precision_test = precision_score(y_test, y_hat_test_clf, pos_label = 'yes') * 100\n",
    "\n",
    "recall_train = recall_score(y_train, y_hat_train_clf, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_hat_test_clf, pos_label = 'yes') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091879e6-ef1c-433b-950b-b26c1695285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 65% and recall = 35% on the training data.\n",
      "Precision = 63% and recall = 34% on the validation data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the training data.\".format(precision_train, recall_train))\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcaf6c-e087-468b-a99e-214d62c3ddb6",
   "metadata": {},
   "source": [
    "##### Then we saved our pipeline for future predictions using dump function in joblib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87704a5-1fc9-4b56-97dc-7d193586826f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf_model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf,'clf_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a29572-b422-44ca-b199-ea06f22a1a73",
   "metadata": {},
   "source": [
    "##### We load our pipeline becuase we wanted to do some prediction on a data which is in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ea612d8-32b8-4c15-9504-fbb49e4a7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scoring script goes here\n",
    "clf_load = joblib.load('clf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4dd9b22-19c2-4259-a821-711d65e31a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1bab71-9115-4f98-b277-cf75049d5ce7",
   "metadata": {},
   "source": [
    "Since we have a json data, we load the data with open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a9bcea-fb78-4e3d-a2b0-29e3fcf745bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': {'0': 40, '1': 47}, 'balance': {'0': 580, '1': 3644}, 'campaign': {'0': 1, '1': 2}, 'contact': {'0': 'unknown', '1': 'unknown'}, 'day': {'0': 16, '1': 9}, 'default': {'0': 'no', '1': 'no'}, 'duration': {'0': 192, '1': 83}, 'education': {'0': 'secondary', '1': 'secondary'}, 'housing': {'0': 'yes', '1': 'no'}, 'job': {'0': 'blue-collar', '1': 'services'}, 'loan': {'0': 'no', '1': 'no'}, 'marital': {'0': 'married', '1': 'single'}, 'month': {'0': 'may', '1': 'jun'}, 'pdays': {'0': -1, '1': -1}, 'poutcome': {'0': 'unknown', '1': 'unknown'}, 'previous': {'0': 0, '1': 0}}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'age': {'0': 40, '1': 47},\n",
       " 'balance': {'0': 580, '1': 3644},\n",
       " 'campaign': {'0': 1, '1': 2},\n",
       " 'contact': {'0': 'unknown', '1': 'unknown'},\n",
       " 'day': {'0': 16, '1': 9},\n",
       " 'default': {'0': 'no', '1': 'no'},\n",
       " 'duration': {'0': 192, '1': 83},\n",
       " 'education': {'0': 'secondary', '1': 'secondary'},\n",
       " 'housing': {'0': 'yes', '1': 'no'},\n",
       " 'job': {'0': 'blue-collar', '1': 'services'},\n",
       " 'loan': {'0': 'no', '1': 'no'},\n",
       " 'marital': {'0': 'married', '1': 'single'},\n",
       " 'month': {'0': 'may', '1': 'jun'},\n",
       " 'pdays': {'0': -1, '1': -1},\n",
       " 'poutcome': {'0': 'unknown', '1': 'unknown'},\n",
       " 'previous': {'0': 0, '1': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening JSON file\n",
    "with open('data/new_data.json', 'r') as openfile:\n",
    " \n",
    "    # Reading from json file\n",
    "    json_object = json.load(openfile)\n",
    "    print(json_object)\n",
    "    print(type(json_object))\n",
    "\n",
    "\n",
    "json_object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719af202-78c9-4f4f-aa1c-3fdf5f0e3b60",
   "metadata": {},
   "source": [
    "##### For using this data in our modeling, first we change the data to dataframe and did predictions using our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbc0517-5bea-4108-91b4-9116968aeb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>campaign</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>default</th>\n",
       "      <th>duration</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>job</th>\n",
       "      <th>loan</th>\n",
       "      <th>marital</th>\n",
       "      <th>month</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16</td>\n",
       "      <td>no</td>\n",
       "      <td>192</td>\n",
       "      <td>secondary</td>\n",
       "      <td>yes</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>no</td>\n",
       "      <td>married</td>\n",
       "      <td>may</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>3644</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>9</td>\n",
       "      <td>no</td>\n",
       "      <td>83</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>services</td>\n",
       "      <td>no</td>\n",
       "      <td>single</td>\n",
       "      <td>jun</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  balance  campaign  contact  day default  duration  education housing  \\\n",
       "0   40      580         1  unknown   16      no       192  secondary     yes   \n",
       "1   47     3644         2  unknown    9      no        83  secondary      no   \n",
       "\n",
       "           job loan  marital month  pdays poutcome  previous  \n",
       "0  blue-collar   no  married   may     -1  unknown         0  \n",
       "1     services   no   single   jun     -1  unknown         0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_data_json = json.loads(new_data)\n",
    "df=pd.DataFrame.from_dict(json_object)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d432ee5-5f93-468a-adca-f3e6e5c5d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no', 'no'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf_load.predict(df)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3870fa-4447-4da6-b80e-dbcea696c717",
   "metadata": {},
   "source": [
    "For saving it as a json file, we change it to list and the used dump function to change it to json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "579f5b7b-c107-4db3-a8f6-0cb93de715c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predict=predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac3e53d-9065-494e-bc08-fc01d2b0f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"no\", \"no\"]\n"
     ]
    }
   ],
   "source": [
    "json_data = json.dumps(list_predict)\n",
    "json_data\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd02ea1-0da3-420e-87c5-e131b4fb4a63",
   "metadata": {},
   "source": [
    "For saving it in a json file format, we used open function to save the file as a json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "349558ed-a9a6-4359-9eee-edba8201df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"json_data.json\"\n",
    "\n",
    "# Write the data to the JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(json_data, json_file)\n",
    "\n",
    "print(\"JSON file created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
